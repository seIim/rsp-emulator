import jax, jax.numpy as jnp, jax.random as random
from flax.training import train_state
from utils import train_test_split
import flax.linen as nn
import pandas as pd
import optax
import tqdm

def generate_sine_series(y, x):
    V_A1, V_phi1, V_A2, V_phi2, V_A3, V_phi3, Period = y
    t = x * Period
    omega = 2 * jnp.pi / Period
    sine_series = (V_A1 * jnp.sin(omega * t + V_phi1) + 
                   V_A2 * jnp.sin(omega * t + V_phi2) + 
                   V_A3 * jnp.sin(omega * t + V_phi3))
    return sine_series

def create_autoregressive_mask(sequence_length):
    """Create an autoregressive mask of shape (1, 1, sequence_length, sequence_length) to use in self-attention."""
    mask = jnp.tril(jnp.ones((sequence_length, sequence_length))).astype(jnp.bool_)  # Ensure mask is binary
    return mask[None, None, :, :]  # Shape: (1, 1, sequence_length, sequence_length)

class TransformerBlock(nn.Module):
    model_dim: int
    num_heads: int
    ff_dim: int

    @nn.compact
    def __call__(self, x, mask=None):
        # Layer normalization
        x = nn.LayerNorm()(x)

        # Multi-head self-attention
        attention = nn.SelfAttention(
            num_heads=self.num_heads,
            qkv_features=self.model_dim,
            use_bias=True,
        )(x, mask=mask)

        x = x + attention
        x = nn.LayerNorm()(x)

        # Feed-forward network
        ff_output = nn.Dense(self.ff_dim)(x)
        ff_output = nn.relu(ff_output)
        ff_output = nn.Dense(self.model_dim)(ff_output)

        return x + ff_output

class Transformer(nn.Module):
    num_layers: int
    model_dim: int
    num_heads: int
    ff_dim: int
    output_dim: int
    sequence_length: int

    @nn.compact
    def __call__(self, x):
        batch_size, feature_dim = x.shape

        # Project static input to match sequence length
        x = nn.Dense(self.model_dim)(x)  # Shape: (batch_size, model_dim)
        x = jnp.expand_dims(x, axis=1)  # Shape: (batch_size, 1, model_dim)
        x = jnp.tile(x, (1, self.sequence_length, 1))  # Shape: (batch_size, sequence_length, model_dim)

        mask = create_autoregressive_mask(self.sequence_length)  # Shape: (1, 1, sequence_length, sequence_length)
        mask = jnp.tile(mask, (batch_size, self.num_heads, 1, 1))  # Shape: (batch_size, num_heads, sequence_length, sequence_length)

        for _ in range(self.num_layers):
            x = TransformerBlock(self.model_dim, self.num_heads, self.ff_dim)(x, mask=mask)

        x = nn.LayerNorm()(x)
        x = nn.Dense(self.output_dim)(x)  # Shape: (batch_size, sequence_length, output_dim)

        return x.squeeze(-1)  # Ensure the final output shape is (batch_size, sequence_length)

def mse_loss(predictions, targets):
    return jnp.mean((predictions.squeeze() - targets.squeeze()) ** 2)

@jax.jit
def train_step(state, batch):
    inputs, targets = batch['inputs'], batch['targets']

    def loss_fn(params):
        # Get the predictions
        predictions = state.apply_fn({'params': params}, inputs)
        # Calculate the loss
        loss = mse_loss(predictions, targets)
        return loss

    grad_fn = jax.value_and_grad(loss_fn)
    loss, grads = grad_fn(state.params)
    state = state.apply_gradients(grads=grads)

    return state, loss

def create_train_state(rng, model, learning_rate, input_dim):
    params = model.init(rng, jnp.ones((1, input_dim), jnp.float32))['params']
    tx = optax.adam(learning_rate)
    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)

# Hyperparameters and data setup
batch_size = 256
input_dim = 5  # Static input dimension
sequence_length = 20  # Length of time series generated by the model
num_epochs = 100
learning_rate = 0.001

df = pd.read_csv('../data/rsp.rrab.dat', sep=r'\s+')
df = df.sample(frac=1.0)
df['V_A2'] = df['V_R21']*df['V_A1']
df['V_A3'] = df['V_R31']*df['V_A1']
df['V_phi2'] = jnp.mod(jnp.array((2*df['V_phi1'] + df['V_P21']).values), 2*jnp.pi)
df['V_phi3'] = jnp.mod(jnp.array((3*df['V_phi1'] + df['V_P31']).values), 2*jnp.pi)

X = ['Z', 'X', 'M', 'L', 'Teff']
y = ['V_A1', 'V_phi1', 'V_A2', 'V_phi2', 'V_A3', 'V_phi3', 'Period']

X = jnp.array(df[X])
y = jnp.array(df[y])
y = jnp.array([generate_sine_series(y, jnp.linspace(0,1,sequence_length)) for y in y])
data = jnp.concatenate([X, y], axis=1)
key1, key2 = random.split(random.PRNGKey(611))
X_train, y_train, X_test, y_test = train_test_split(key1, X, y, split=0.2, shuffle=True)
inputs = X_train
targets = y_train
# Model initialization
model = Transformer(num_layers=4, model_dim=128, num_heads=8, ff_dim=512, output_dim=1, sequence_length=sequence_length)
state = create_train_state(key2, model, learning_rate=learning_rate, input_dim=input_dim)

# Training loop

def batch_generator(X, y, batch_size):
    num_batches = X.shape[0] // batch_size
    indices = jnp.arange(X.shape[0])
    for i in range(num_batches):
        batch_indices = indices[i*batch_size:(i+1)*batch_size]
        yield {'inputs': X[batch_indices], 'targets': y[batch_indices]}
test_loss = 0
@jax.jit
def test_loss_fn(state, inputs):
    predictions = state.apply_fn({'params': state.params}, inputs)
    loss = mse_loss(predictions, y_test)
    return loss

with tqdm.trange(100) as t:
    for epoch in t:
        epoch_loss = []
        for batch in batch_generator(X_train, y_train, batch_size):
            state, loss = train_step(state, batch)
            epoch_loss.append(loss) 
        test_loss = test_loss_fn(state, X_test)
        t.set_postfix_str(f"train: {jnp.mean(jnp.asarray(epoch_loss)):.4f}, test: {test_loss:.4f}", refresh=False)


predictions = state.apply_fn({'params': state.params}, X_train)
import matplotlib.pyplot as plt
for i in range(10):
    plt.plot(jnp.linspace(0,1,sequence_length),predictions[i])
    plt.plot(jnp.linspace(0,1,sequence_length),y_train[i])
    plt.savefig(f'../figs/train_preds_{i}.pdf', bbox_inches='tight')
    plt.show()

#for epoch in range(num_epochs):
#    batch = {'inputs': inputs, 'targets': targets}
#    state, loss = train_step(state, batch)
#    print(f"Epoch {epoch}, Loss: {loss}")
